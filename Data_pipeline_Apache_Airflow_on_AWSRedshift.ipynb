{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUTORIAL NOTEBOOK \n",
    "# HOW TO BUILD AN ETL DATA PIPELINE HOSTED ON AMAZON REDSHIFT WITH APACHE AIRFLOW \n",
    "\n",
    "\n",
    "### by Tran Nguyen\n",
    "\n",
    "## Table of Contents\n",
    "- [1. Introduction](#introduction)\n",
    "- [2. Practice - process of building a data pipeline hosted on Amazon Redshift using Apache Airflow](#process)\n",
    "    + [2.1. Build and launch a Redshift cluster using IaC](#launcher)\n",
    "    + [2.2. Set up credentials an connection to Redshift cluster for Apache Airflow](#airflow_connection)\n",
    "    + [2.3. Create and run an Airflow ETL DAG](#run-dag)\n",
    "    + [2.4. Clean up the resources](#clean-up)\n",
    "- [3. Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1. INTRODUCTION\n",
    "\n",
    "### 1.1. AMAZON REDSHIFT\n",
    "\n",
    "- Amazon Redshift is a cloud-managed, column-oriented, massively parallel processing (MPP) database. Redshift is considered as a modified postgresql database, in which, table is partitioned up into many pieces and distributed across slices in different machines. \n",
    "- Some good resources for learning Redshift:\n",
    "    + https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\n",
    "\n",
    "    + https://aws.amazon.com/blogs/big-data/how-i-built-a-data-warehouse-using-amazon-redshift-and-aws-services-in-record-time/\n",
    "\n",
    "    + https://panoply.io/data-warehouse-guide/redshift-etl/\n",
    "\n",
    "    + https://d1.awsstatic.com/whitepapers/enterprise-data-warehousing-on-aws.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. AIRFLOW COMPONENTS\n",
    "\n",
    "- Apache Airflow is one of the best workflow management systems (WMS) that provides data engineers a friendly platform to automate, monitor and maintain their complex data pipelines. Started at Airbnb in 2014, then became an open-source project with an excellent UI, Airflow has become a popular choice among developers.\n",
    "\n",
    "- 5 main components of Airflow:\n",
    "    - Scheduler: Starts DAGs based on triggers or schedules and moves them towards completion.\n",
    "    - Work Queue: is used by the scheduler in most Airflow installations to deliver tasks that need to be run to the Workers.\n",
    "    - Workers: Runs and records the outcome of individual pipeline tasks.\n",
    "    - Database/Metadata database: saves credentials, connections, history, configuration, also stores the state of all tasks in the system.\n",
    "    - UI/Web Server: Provides a control interface for users and maintainers.\n",
    "- Airflow could be connected to external systems and databases through a reusable interface called Airflow Hooks. There are many available hooks for different system. The PostgresHook is used to connect Airflow to Amazon Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. USE CASE OF AIRFLOW & AMAZON REDSHIFT\n",
    "\n",
    "- A standard ETL data pipeline hosted on Amazon Redshift could be a workflow that extracts the data from S3, stages them in Redshift, and transforms data into a set of dimensional tables.\n",
    "- This whole ETL pipeline could be implement as a DAG on Apache Airflow, so that the data pipeline can be automatically monitored and maintained.\n",
    "- In this tutorial, I will walk you through the whole process of building a data pipeline hosted on Amazon Redshift using Apache Airflow.\n",
    "- The process include 4 steps. Step 1 & 4 could be done through this jupyter notebook:\n",
    "    + Step 1: Build and launch a Redshift cluster using IaC (Infrastructure-as-code).\n",
    "    + Step 2: Set up credentials an connection to Redshift cluster for Apache Airflow\n",
    "    + Step 3: Run an Airflow DAG which is a simple ETL data pipeline hosted on Redshift\n",
    "    + Step 4: Clean up the AWS resources using IaC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='process'></a>\n",
    "## 2. PRACTICE - PROCESS OF BUILDING A DATA PIPELINE HOSTED ON AMAZON REDSHIFT USING APACHE AIRFLOW\n",
    "<a id='launcher'></a>\n",
    "### 2.1. STEP 1: BUILD AND LAUNCH A REDSHIFT CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "### Import neccessary package\n",
    "import boto3\n",
    "import json\n",
    "import configparser\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. GENERATE AN IAM USER WITH AWS SECRET AND ACCESS KEY\n",
    "\n",
    "- Create a new IAM user in the AWS account\n",
    "- Give it `AdministratorAccess`, From `Attach existing policies directly` Tab\n",
    "- Take note of the access key and secret \n",
    "- Edit the file `dwh.cfg` in the same folder as this notebook and fill\n",
    "<font color='red'>\n",
    "<BR>\n",
    "[AWS]<BR>\n",
    "KEY= YOUR_AWS_KEY<BR>\n",
    "SECRET= YOUR_AWS_SECRET<BR>\n",
    "</font>\n",
    "\n",
    "- This file contains all the parameters for creating the AWS Redshift cluster. We can modify the information if neccessary, such as cluster_type, user_id, password, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load DWH Params from the file `dwh.cfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwhCluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param       Value\n",
       "0  DWH_CLUSTER_TYPE        multi-node\n",
       "1  DWH_NUM_NODES           4         \n",
       "2  DWH_NODE_TYPE           dc2.large \n",
       "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
       "4  DWH_DB                  dwh       \n",
       "5  DWH_DB_USER             dwhuser   \n",
       "6  DWH_DB_PASSWORD         Passw0rd  \n",
       "7  DWH_PORT                5439      \n",
       "8  DWH_IAM_ROLE_NAME       dwhRole   "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. CREATE CLIENTS FOR IAM, EC2, S3 AND REDSHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choosing an AWS region \n",
    "target_region = \"us-west-2\"\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name = target_region,\n",
    "                       aws_access_key_id = KEY,\n",
    "                       aws_secret_access_key = SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name = target_region,\n",
    "                       aws_access_key_id = KEY,\n",
    "                       aws_secret_access_key = SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                       region_name = target_region,\n",
    "                       aws_access_key_id = KEY,\n",
    "                         aws_secret_access_key = SECRET     \n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name = target_region,\n",
    "                       aws_access_key_id = KEY,\n",
    "                       aws_secret_access_key = SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. CREATE IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::543309725137:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "### Create the role\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4. CREATE REDSHIFT CLUSTER\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType = DWH_CLUSTER_TYPE,\n",
    "        NodeType = DWH_NODE_TYPE,\n",
    "        NumberOfNodes = int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName = DWH_DB,\n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername = DWH_DB_USER,\n",
    "        MasterUserPassword = DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe the cluster to see its status\n",
    "- Run the block below several times until the cluster status becomes `Available` (around 5-10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data = x, columns = [\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier = DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take note of the cluster <font color='red'> endpoint and role ARN </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN THIS unless the cluster status becomes \"Available\"\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5. OPEN AN INCOMING TCP PORT TO ACCESS THE CLUSTER ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id = myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName = defaultSg.group_name,\n",
    "        CidrIp = '0.0.0.0/0',\n",
    "        IpProtocol = 'TCP',\n",
    "        FromPort = int(DWH_PORT),\n",
    "        ToPort = int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6. VALIDATE THE CONNECTION TO THE CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='airflow_connection'></a>\n",
    "### 2.2. SET UP CREDENTIALS & CONNECTION TO AWS REDSHIFT CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. SET UP CREDENTIALS\n",
    "On the Apache Airflow UI:\n",
    "\n",
    "1. Open Admin -> Connections\n",
    "2. Click \"Create\"\n",
    "3. Set \"Conn Id\" to \"aws_credentials\", \"Conn Type\" to \"Amazon Web Services\"\n",
    "4. Set \"Login\" to your aws_access_key_id and \"Password\" to your aws_secret_key\n",
    "5. Click save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/aws_credentials.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.  SET UP CONNECTION TO REDSHIFT\n",
    "On the Apache Airflow UI:\n",
    "\n",
    "1. Open Admin -> Connections\n",
    "2. Click \"Create\"\n",
    "3. Fill in the information as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conn Id :: redshift\n",
      "Conn Id :: Postgres\n",
      "Host ::dwhcluster.cscn3pfaocgv.us-west-2.redshift.amazonaws.com\n",
      "Schema ::dwh        \n",
      "Login :: dwhuser\n",
      "Password ::Passw0rd\n",
      "Port ::5439\n"
     ]
    }
   ],
   "source": [
    "print(f\"Conn Id :: redshift\\nConn Id :: Postgres\\nHost ::{DWH_ENDPOINT}\\nSchema ::{DWH_DB}\\\n",
    "        \\nLogin :: {DWH_DB_USER}\\nPassword ::{DWH_DB_PASSWORD}\\nPort ::{DWH_PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/redshift.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run-dag'></a>\n",
    "### 2.3. CREATE AND RUN AN AIRLOW ETL DAG\n",
    "\n",
    "- This example uses the biking data stored on an public s3 bucket `\"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"` from Udacity. \n",
    "- In this example, data from the file `divvy_trips_2018.csv` will be loaded to Redshift as a stanging table called `trips`, then the data from `trips` will be extracted to create a traffic analysis table called `station_traffic`, where `num_departures` and `num_arrivals` are counted based on the total number of `from_station_id` and `to_station_id`, respectively, from the `trips` table.\n",
    "\n",
    "<img src=\"biking_erd.png\" width=\"50%\"/>\n",
    "\n",
    "(ERD diagram was made by using https://dbdiagram.io/)\n",
    "\n",
    "\n",
    "- Below is the code for this end-to-end data pipeline, which was implemeted as an Airflow DAG. This code could be used to create a script file for running on Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "##### ------ Define python function -------\n",
    "def load_data_to_redshift(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to load data from s3 to redshift using the copy command\n",
    "    \"\"\"\n",
    "    aws_hook = AwsHook(\"aws_credentials\") #from Step 2.2.1. Set up credentials\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\") #from Step 2.2.2. Set up connection to redshift\n",
    "    ## print info to make sure we get the right access_key and secret_key, since\n",
    "    # it often gets wrong during copy/paste\n",
    "    logging.info(sql_statements.COPY_ALL_TRIPS_SQL.format(credentials.access_key, credentials.secret_key))\n",
    "    ## Run the copy command\n",
    "    redshift_hook.run(sql_statements.COPY_ALL_TRIPS_SQL.format(credentials.access_key, credentials.secret_key))\n",
    "\n",
    "##### ------ Define SQL statements -------\n",
    "\n",
    "CREATE_TRIPS_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS trips (\n",
    "    trip_id INTEGER NOT NULL,\n",
    "    start_time TIMESTAMP NOT NULL,\n",
    "    end_time TIMESTAMP NOT NULL,\n",
    "    bikeid INTEGER NOT NULL,\n",
    "    tripduration DECIMAL(16,2) NOT NULL,\n",
    "    from_station_id INTEGER NOT NULL,\n",
    "    from_station_name VARCHAR(100) NOT NULL,\n",
    "    to_station_id INTEGER NOT NULL,\n",
    "    to_station_name VARCHAR(100) NOT NULL,\n",
    "    usertype VARCHAR(20),\n",
    "    gender VARCHAR(6),\n",
    "    birthyear INTEGER,\n",
    "    PRIMARY KEY(trip_id))\n",
    "    DISTSTYLE ALL;\n",
    "    \"\"\"\n",
    "\n",
    "COPY_SQL = \"\"\"\n",
    "    COPY {}\n",
    "    FROM '{}'\n",
    "    ACCESS_KEY_ID '{{}}'\n",
    "    SECRET_ACCESS_KEY '{{}}'\n",
    "    IGNOREHEADER 1\n",
    "    DELIMITER ','\n",
    "    \"\"\"\n",
    "\n",
    "COPY_ALL_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"\n",
    ")\n",
    "\n",
    "STATION_TRAFFIC_SQL = \"\"\"\n",
    "    DROP TABLE IF EXISTS station_traffic;\n",
    "    CREATE TABLE station_traffic AS\n",
    "    SELECT\n",
    "        DISTINCT(t.from_station_id) AS station_id,\n",
    "        t.from_station_name AS station_name,\n",
    "        num_departures,\n",
    "        num_arrivals\n",
    "    FROM trips t\n",
    "    JOIN (\n",
    "        SELECT\n",
    "            from_station_id,\n",
    "            COUNT(from_station_id) AS num_departures\n",
    "        FROM trips\n",
    "        GROUP BY from_station_id\n",
    "    ) AS s1 ON t.from_station_id = s1.from_station_id\n",
    "    JOIN (\n",
    "        SELECT\n",
    "            to_station_id,\n",
    "            COUNT(to_station_id) AS num_arrivals\n",
    "        FROM trips\n",
    "        GROUP BY to_station_id\n",
    "    ) AS s2 ON t.from_station_id = s2.to_station_id\n",
    "    \"\"\"\n",
    "\n",
    "##### ------ Define dag -------\n",
    "dag = DAG(\n",
    "    'data_pipeline_airflow_Redshift',\n",
    "    start_date = datetime.datetime.now()\n",
    ")\n",
    "\n",
    "##### ------ Define tasks -------\n",
    "### --- Task with PostgresOperator ---\n",
    "# Create table\n",
    "create_table = PostgresOperator(\n",
    "    task_id = \"create_table\",\n",
    "    dag = dag,\n",
    "    postgres_conn_id = \"redshift\",\n",
    "    sql = sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "# Traffic analysis\n",
    "location_traffic_task = PostgresOperator(\n",
    "    task_id = \"calculate_location_traffic\",\n",
    "    dag = dag,\n",
    "    postgres_conn_id = \"redshift\",\n",
    "    sql = sql_statements.STATION_TRAFFIC_SQL\n",
    ")\n",
    "\n",
    "### --- Task with PythonOperator ---\n",
    "copy_task = PythonOperator(\n",
    "    task_id = 'load_from_s3_to_redshift',\n",
    "    dag = dag,\n",
    "    python_callable = load_data_to_redshift\n",
    ")\n",
    "\n",
    "\n",
    "##### ------ Configure the task dependencies -------\n",
    "# Task dependencies such that the graph looks like the following:\n",
    "# create_table -> copy_task -> location_traffic_task\n",
    "\n",
    "create_table >> copy_task\n",
    "copy_task >> location_traffic_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean-up'></a>\n",
    "### 2.4. CLEAN UP THE RESOURCES\n",
    "DO NOT RUN THIS UNLESS YOU ARE SURE TO DELETE YOUR CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources, don't skip any final snapshot to avoid AWS fee\n",
    "redshift.delete_cluster(ClusterIdentifier = DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the block below several times until the cluster was really deleted. When the cluster was deleted, there would be no result at all:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName = DWH_IAM_ROLE_NAME, PolicyArn = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName = DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of the process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## 3. CONCLUSIONS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tutorial is a demo for creating a simple data warehouse ETL pipeline on AWS with Airflow. I hope it is helpful.\n",
    "- Some of the materials are from the Data Engineering nanodegree program on Udacity.\n",
    "- This is a part of my medium blog post, check out if you want to learn more about it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
